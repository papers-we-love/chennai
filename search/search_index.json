{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Welcome to the Chennai chapter of Papers We Love . Papers We Love (PWL) is a community built around reading, discussing and learning more about academic computer science papers. We hold regular meetups in Chennai (and online). We select a paper every month. During the meetup, someone will present the paper, followed by a discussion. These meetings are open for all and completely free to attend. Upcoming Meetup Date: 21st November 2020 (Saturday) Time: 6:00 PM to 7:30 PM Venue: Google Meet Paper: A Brief Survey of Deep Reinforcement Learning RSVP Here Where to go from here Take a look at the Code of Conduct There's a summary of the past meetups we've had so far Check the Resources section for more papers and other useful tools. Contact Arvind Nedumaran : Email | Twitter Chapter Email : chennaipwl@gmail.com Papers We Love contact@paperswelove.org","title":"Welcome"},{"location":"#welcome","text":"Welcome to the Chennai chapter of Papers We Love . Papers We Love (PWL) is a community built around reading, discussing and learning more about academic computer science papers. We hold regular meetups in Chennai (and online). We select a paper every month. During the meetup, someone will present the paper, followed by a discussion. These meetings are open for all and completely free to attend.","title":"Welcome"},{"location":"#upcoming-meetup","text":"Date: 21st November 2020 (Saturday) Time: 6:00 PM to 7:30 PM Venue: Google Meet Paper: A Brief Survey of Deep Reinforcement Learning RSVP Here","title":"Upcoming Meetup"},{"location":"#where-to-go-from-here","text":"Take a look at the Code of Conduct There's a summary of the past meetups we've had so far Check the Resources section for more papers and other useful tools.","title":"Where to go from here"},{"location":"#contact","text":"Arvind Nedumaran : Email | Twitter Chapter Email : chennaipwl@gmail.com Papers We Love contact@paperswelove.org","title":"Contact"},{"location":"coc/","text":"Code of Conduct All attendees, speakers, exhibitors, organizers, contributors, and volunteers are required to conform to the following Code of Conduct. Papers We Love events are for anyone interested in Computer Science/Computer Engineering, its history, and related fields to discuss academic research in a fun, engaging, and respectful environment. Be an adult, don't be a jerk. We value the participation of each member of the community and want all attendees to have an enjoyable and fulfilling experience. Accordingly, all attendees are expected to show respect and courtesy to other attendees throughout the meet-ups and at all Papers We Love events and interactions on the GitHub repository, IRC, or Slack channels. Need help? If you are experiencing harassment on or have concerns about the content within the GitHub repo , the #paperswelove IRC channel on Freenode, the paperswelove.slack.com Slack, or PapersWeLove.org please contact: Arvind Nedumaran : arvindamirtaa@gmail.com Ganesh Srinivas : gs401 [at] snu [dot] in Benoy Thomas John : benoytj [at] gmail [dot] com Chapter Email : chennaipwl@gmail.com All contact@paperswelove.org The organizers of your local Papers We Love meet-up/event are available to help you with any issues or concerns at live events. What it means Papers We Love is dedicated to providing a harassment-free experience for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of meetup participants in any form. All communication should be appropriate for a professional audience including people of many different backgrounds. English is the preferred language of communication. Sexual language and imagery are not appropriate for any meet-up, including talks. Be kind to others. Do not insult or put down other attendees. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for Papers We Love. Attendees violating these rules may be asked to leave the meet-up at the sole discretion of the organizers. Thank you for helping make this a welcoming, friendly event for all. Spelling it out Harassment includes offensive verbal comments related to gender, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. Contributors to the GitHub repository, the Meetup and/or event-related sites, sponsors, or similar are also subject to the anti-harassment policy. Organizers (including volunteers) should not use sexualized clothing/uniforms/costumes or otherwise create a sexualized environment.","title":"Code of Conduct"},{"location":"coc/#code-of-conduct","text":"All attendees, speakers, exhibitors, organizers, contributors, and volunteers are required to conform to the following Code of Conduct. Papers We Love events are for anyone interested in Computer Science/Computer Engineering, its history, and related fields to discuss academic research in a fun, engaging, and respectful environment. Be an adult, don't be a jerk. We value the participation of each member of the community and want all attendees to have an enjoyable and fulfilling experience. Accordingly, all attendees are expected to show respect and courtesy to other attendees throughout the meet-ups and at all Papers We Love events and interactions on the GitHub repository, IRC, or Slack channels.","title":"Code of Conduct"},{"location":"coc/#need-help","text":"If you are experiencing harassment on or have concerns about the content within the GitHub repo , the #paperswelove IRC channel on Freenode, the paperswelove.slack.com Slack, or PapersWeLove.org please contact: Arvind Nedumaran : arvindamirtaa@gmail.com Ganesh Srinivas : gs401 [at] snu [dot] in Benoy Thomas John : benoytj [at] gmail [dot] com Chapter Email : chennaipwl@gmail.com All contact@paperswelove.org The organizers of your local Papers We Love meet-up/event are available to help you with any issues or concerns at live events.","title":"Need help?"},{"location":"coc/#what-it-means","text":"Papers We Love is dedicated to providing a harassment-free experience for everyone, regardless of gender, sexual orientation, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of meetup participants in any form. All communication should be appropriate for a professional audience including people of many different backgrounds. English is the preferred language of communication. Sexual language and imagery are not appropriate for any meet-up, including talks. Be kind to others. Do not insult or put down other attendees. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for Papers We Love. Attendees violating these rules may be asked to leave the meet-up at the sole discretion of the organizers. Thank you for helping make this a welcoming, friendly event for all.","title":"What it means"},{"location":"coc/#spelling-it-out","text":"Harassment includes offensive verbal comments related to gender, sexual orientation, disability, physical appearance, body size, race, religion, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants asked to stop any harassing behavior are expected to comply immediately. Contributors to the GitHub repository, the Meetup and/or event-related sites, sponsors, or similar are also subject to the anti-harassment policy. Organizers (including volunteers) should not use sexualized clothing/uniforms/costumes or otherwise create a sexualized environment.","title":"Spelling it out"},{"location":"events/events/","text":"Past Events 18 March 2017 - An Industrial-Strength Audio Search Algorithm 17 March 2018 - Deep Learning (Review) 21 April 2018 - Multiworld Testing Decision Service: A System for Experimentation, Learning, And Decision-Making 19 Jan 2019(tentative) - Vuvuzela: Scalable Private Messaging Resistant to Traffic Analysis 21 November 2020 - Statistical Modeling: The Two Cultures by Leo Breiman","title":"Past Events"},{"location":"events/events/#past-events","text":"18 March 2017 - An Industrial-Strength Audio Search Algorithm 17 March 2018 - Deep Learning (Review) 21 April 2018 - Multiworld Testing Decision Service: A System for Experimentation, Learning, And Decision-Making 19 Jan 2019(tentative) - Vuvuzela: Scalable Private Messaging Resistant to Traffic Analysis 21 November 2020 - Statistical Modeling: The Two Cultures by Leo Breiman","title":"Past Events"},{"location":"papers/brief-survey-of-deep-reinforcement-learning/","text":"A Brief Survey of Deep Reinforcement Learning Author(s): Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath Abstract: Abstract\u2014Deep reinforcement learning is poised to revolu-tionise the field of AI and represents a step towards buildingautonomous systems with a higher level understanding of thevisual world. Currently, deep learning is enabling reinforcementlearning to scale to problems that were previously intractable,such as learning to play video games directly from pixels. Deepreinforcement learning algorithms are also applied to robotics,allowing control policies for robots to be learned directly fromcamera inputs in the real world. In this survey, we begin withan introduction to the general field of reinforcement learning,then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms indeep reinforcement learning, including the deepQ-network,trust region policy optimisation, and asynchronous advantageactor-critic. In parallel, we highlight the unique advantages ofdeep neural networks, focusing on visual understanding viareinforcement learning. To conclude, we describe several currentareas of research within the field. Read the full paper here .","title":"A Brief Survey of Deep Reinforcement Learning"},{"location":"papers/brief-survey-of-deep-reinforcement-learning/#a-brief-survey-of-deep-reinforcement-learning","text":"Author(s): Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath Abstract: Abstract\u2014Deep reinforcement learning is poised to revolu-tionise the field of AI and represents a step towards buildingautonomous systems with a higher level understanding of thevisual world. Currently, deep learning is enabling reinforcementlearning to scale to problems that were previously intractable,such as learning to play video games directly from pixels. Deepreinforcement learning algorithms are also applied to robotics,allowing control policies for robots to be learned directly fromcamera inputs in the real world. In this survey, we begin withan introduction to the general field of reinforcement learning,then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms indeep reinforcement learning, including the deepQ-network,trust region policy optimisation, and asynchronous advantageactor-critic. In parallel, we highlight the unique advantages ofdeep neural networks, focusing on visual understanding viareinforcement learning. To conclude, we describe several currentareas of research within the field. Read the full paper here .","title":"A Brief Survey of Deep Reinforcement Learning"},{"location":"papers/deep_learning_review/","text":"Deep Learning (Review) Papers We Love Chennai Meetup #2 Location : Viasat India Private Limited, 5th Floor, Block C, SP Infocity, Kandanchavadi, Perungudi, Chennai. Date and Time : Saturday, 17 March 2018 from 3pm to 4.30pm. Read the paper .","title":"Deep Learning (Review)"},{"location":"papers/deep_learning_review/#deep-learning-review","text":"Papers We Love Chennai Meetup #2 Location : Viasat India Private Limited, 5th Floor, Block C, SP Infocity, Kandanchavadi, Perungudi, Chennai. Date and Time : Saturday, 17 March 2018 from 3pm to 4.30pm. Read the paper .","title":"Deep Learning (Review)"},{"location":"papers/inustrial_strength_audio_search/","text":"An Industrial Strength Audio Search Algorithm Papers We Love Chennai Meetup #1 Location : Seminar Hall at Chennai Mathematical Institute, Siruseri, Chennai ( directions ) Date and Time : Saturday, 18 March 2017 from 4pm to 5pm. Read the paper . Presented by Ganesh Srinivas.","title":"An Industrial Strength Audio Search Algorithm"},{"location":"papers/inustrial_strength_audio_search/#an-industrial-strength-audio-search-algorithm","text":"Papers We Love Chennai Meetup #1 Location : Seminar Hall at Chennai Mathematical Institute, Siruseri, Chennai ( directions ) Date and Time : Saturday, 18 March 2017 from 4pm to 5pm. Read the paper . Presented by Ganesh Srinivas.","title":"An Industrial Strength Audio Search Algorithm"},{"location":"papers/miltiworld-testing-decision-service/","text":"Multiworld Testing Decision Service Papers We Love Chennai Meetup #3 Location : Viasat India Private Limited, 5th Floor, Block C, SP Infocity, Kandanchavadi, Perungudi, Chennai. Date and Time : Saturday, 21 April 2018 from 3pm to 4.30pm. Read the paper . Slides: http://hunch.net/~rwil/, http://hunch.net/~rwil/Motivation_algs_theory.pptx Presented by Ganesh Srinivas","title":"Multiworld Testing Decision Service"},{"location":"papers/miltiworld-testing-decision-service/#multiworld-testing-decision-service","text":"Papers We Love Chennai Meetup #3 Location : Viasat India Private Limited, 5th Floor, Block C, SP Infocity, Kandanchavadi, Perungudi, Chennai. Date and Time : Saturday, 21 April 2018 from 3pm to 4.30pm. Read the paper . Slides: http://hunch.net/~rwil/, http://hunch.net/~rwil/Motivation_algs_theory.pptx Presented by Ganesh Srinivas","title":"Multiworld Testing Decision Service"},{"location":"papers/statistical-modelling-two-cultures-leo-breiman/","text":"Statistical Modeling: The Two Cultures Author(s): Leo Breiman Abstract: There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools. Read the full paper here . Note: The above link also contains comments on the paper from notable statisticians. Although not necessary for the purpose of discussion at the meetup, reading them is highly encouraged.","title":"Statistical Modeling: The Two Cultures"},{"location":"papers/statistical-modelling-two-cultures-leo-breiman/#statistical-modeling-the-two-cultures","text":"Author(s): Leo Breiman Abstract: There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools. Read the full paper here . Note: The above link also contains comments on the paper from notable statisticians. Although not necessary for the purpose of discussion at the meetup, reading them is highly encouraged.","title":"Statistical Modeling: The Two Cultures"},{"location":"papers/us_learning_semantic_audio_rep/","text":"Unsupervised Learning of Semantic Audio Representations Papers We Love Chennai #4 Location : TBD Date and Time : Saturday, 19 May 2018 3pm to 4.30pm (tentative) Read the paper Abstract Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance. Related work : triplet loss learning (FaceNet paper, Colin Raffel's pairwise sequence embedding for matching MIDIs with MP3s, etc.). AudioSet dataset and construction of VGGish model. DCASE 2018 competition tracks for audio machine learning.","title":"Unsupervised Learning of Semantic Audio Representations"},{"location":"papers/us_learning_semantic_audio_rep/#unsupervised-learning-of-semantic-audio-representations","text":"Papers We Love Chennai #4 Location : TBD Date and Time : Saturday, 19 May 2018 3pm to 4.30pm (tentative) Read the paper","title":"Unsupervised Learning of Semantic Audio Representations"},{"location":"papers/us_learning_semantic_audio_rep/#abstract","text":"Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance. Related work : triplet loss learning (FaceNet paper, Colin Raffel's pairwise sequence embedding for matching MIDIs with MP3s, etc.). AudioSet dataset and construction of VGGish model. DCASE 2018 competition tracks for audio machine learning.","title":"Abstract"},{"location":"resources/resources/","text":"Resources","title":"Resources"},{"location":"resources/resources/#resources","text":"","title":"Resources"}]}